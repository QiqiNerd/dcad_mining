---
title: "dcad_analysis"
author: "Qiqi Chen"
date: "2024-12-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Before you go
rm(list=ls())
```


## Construct the dynamic network

This part constructs a dynamic network of defense cooperation agreements (DCA) and integrates military spending as a node attribute. It uses temporal data to represent agreements as edges that span their active durations. Nodes represent countries, and their military spending as a percentage of GDP is dynamically updated for each year from 1980 to 2010.

### Clean data and initialize

```{r}
library(dplyr)
library(networkDynamic)
library(ndtv)

# Load cleaned datasets
military_spending <- read.csv("military_spending_cleaned.csv")   # Military spending data
dcad <- read.csv("DCAD_endyear_cleaned.csv")   # Defense cooperation agreements data

# Filter data to ensure consistency with the 1980-2010 range
military_spending <- military_spending %>% filter(Year >= 1980 & Year <= 2010)
dcad <- dcad %>% filter(signYear >= 1980 & endYearEstimate <= 2010)

# Create a global mapping of all unique nodes (countries)
all_nodes <- unique(c(dcad$country1, dcad$country2))
node_mapping <- data.frame(name = all_nodes, id = seq_along(all_nodes))

# Initialize the base network with nodes and set node names
base_network <- network.initialize(n = length(all_nodes), directed = FALSE)
base_network %v% "vertex.names" <- node_mapping$name
# Convert base network to a dynamic one
dynamic_network <- networkDynamic(base.net = base_network)
```


### Construct complete network by adding nodes and edges

```{r}
# Add edges and node attributes for each year from 1980 to 2010
for (year in 1980:2010) {
  # Filter agreements active in the current year
  edge_data <- dcad %>% filter(signYear <= year & endYearEstimate >= year)
  
  # Map agreements to edge IDs using the global node mapping
  edge_list <- edge_data %>%
    mutate(
      tail = node_mapping$id[match(country1, node_mapping$name)],
      head = node_mapping$id[match(country2, node_mapping$name)]
    )
  
  # Aggregate duplicate edges and calculate their weights
  edges <- edge_list %>%
    group_by(tail, head) %>%
    summarise(
      weight = n(),
      .groups = "drop"
    )
  
  # Filter military spending data for the current year
  node_data <- military_spending %>% filter(Year == year)
  node_ids <- node_mapping$id[match(node_data$country, node_mapping$name)]
  
  # Add military spending as a dynamic node attribute
  activate.vertex.attribute(
    dynamic_network, 
    prefix = "military_spending_to_GDP",
    value = node_data$military_spending_to_GDP,    # Attribute value
    onset = year,                                  # Start year of attribute validity
    terminus = year + 1,                           # End year of attribute validity
    v = node_ids                                   # Nodes to which the attribute applies
  )
  
  # Add edges to the dynamic network
  add.edges.active(
    dynamic_network, 
    tail = edges$tail,       # Tail nodes
    head = edges$head,       # Head nodes
    onset = year,            # Start year of edge activity
    terminus = year + 1      # End year of edge activity
  )
  
  # Add weights as a dynamic edge attribute
  activate.edge.attribute(
    dynamic_network, 
    prefix = "weight", 
    value = edges$weight,   # Edge weights
    onset = year,           # Start year of weight validity
    terminus = year + 1     # End year of weight validity
  )
  
}
```

### Animation of the dynamic network

This section of the script performs two main tasks:
- It uses compute.animation to optimize the layout of a dynamic network over time based on the Kamada-Kawai algorithm and edge weights.
- It generates an interactive HTML animation using render.d3movie, displaying dynamic node and edge attributes, including node colors, sizes, and edge thickness.

```{r}
# Optimize the dynamic network layout using `compute.animation`
# Will take some time
compute.animation(
  net = dynamic_network,    # The dynamic network object to analyze
  slice.par = list(         # Parameters for slicing the network over time
    start = 1980,            # Start year of the animation
    end = 2010,              # End year of the animation
    interval = 1,            # Time interval for each frame
    aggregate.dur = 1,       # Aggregation duration for each frame
    rule = "latest"          # Use the latest data for each time slice
  ),
  animation.mode = "kamadakawai", # Use the Kamada-Kawai layout algorithm
  seed.coords = matrix(runif(network.size(dynamic_network) * 2), ncol = 2), # Random initial coordinates
  # seed.coords = NULL,            # 初始节点位置（默认随机）
  layout.par = list(),          # Additional parameters for the layout algorithm
  default.dist = NULL,          # Default distance between nodes (if needed)
  weight.attr = "weight",       # Use edge weights for layout calculation
  weight.dist = FALSE,          # Treat weights as similarities (larger values mean closer nodes)
  chain.direction = "forward",  # Compute layouts in forward temporal order
  verbose = TRUE                # Print process information
)

# Specify visualization options for the HTML animation
d3_options <- list(
  playControls = TRUE,       # Show playback controls
  slider = TRUE,             # Include a time slider
  animateOnLoad = TRUE,      # Start animation automatically on page load
  nodeSizeFactor = 0.02      # Adjust node size scaling factor
)

# Generate an interactive HTML animation
# Will take some time
render.d3movie(
  dynamic_network,
  filename = "network_animation_with_labels_final.html", # Output file name
  render.par = list(                                  # Parameters for animation rendering
    tween.frames = 10,                  # Number of interpolated frames between slices
    show.time = TRUE,                   # Display the time labels
    initial.coords = matrix(0, ncol = 2, nrow = network.size(dynamic_network)) # Initial coordinates
  ),
  plot.par = list(
    vertex.col = function(slice, onset, terminus, ...) {
      # Assign node colors based on military spending (% of GDP)
      gdp_values <- get.vertex.attribute.active(slice, "military_spending_to_GDP", at = onset)
      sapply(gdp_values, function(gdp) {
        if (is.na(gdp)) return("gray")     # Nodes with missing data are gray
        if (gdp > 5) return("red")         # High military expenditure: red
        if (gdp > 2) return("orange")      # Medium military expenditure: orange
        return("green")                    # Low military expenditure: green
      })
    },
    vertex.cex = function(slice, onset, terminus, ...) {
      # Scale node sizes based on military spending (% of GDP)
      gdp_values <- get.vertex.attribute.active(slice, "military_spending_to_GDP", at = onset)
      min_size <- 0.3
      max_size <- 2
      gdp_normalized <- scales::rescale(gdp_values, to = c(min_size, max_size), na.rm = TRUE)
      ifelse(is.na(gdp_normalized), min_size, gdp_normalized)   # Default size for missing data
    },
    edge.col = function(slice, onset, terminus, ...) {
      # Assign edge colors based on weight
      edge_weights <- get.edge.attribute.active(slice, "weight", at = onset)
      sapply(edge_weights, function(weight) {
        if (weight > 1) return("#F08080")  # Edges with weight > 1 are light red
        return("gray")                     # Other edges are gray
      })
    },
    edge.lwd = function(slice, onset, terminus, ...) {
       # Adjust edge thickness based on weight
      edge_weights <- get.edge.attribute.active(slice, "weight", at = onset)
      scales::rescale(edge_weights, to = c(0.5, 5), na.rm = TRUE)  # Rescale weights for line width
    },
    displaylabels = TRUE  # Display node labels
  ),
  vertex.tooltip = function(slice, onset, terminus, ...) {
    # Create tooltips for nodes based on military spending (% of GDP)
    gdp_values <- get.vertex.attribute.active(slice, "military_spending_to_GDP", at = onset)
    paste0(
      "Military expenditure (% of GDP): ", ifelse(is.na(gdp_values), "N/A", gdp_values), "<br>"
    )
  },
  label = function(slice, ...) {
    network.vertex.names(slice)  # Use node names as labels
  },
  edge.tooltip = function(slice, onset, terminus, ...) {
    # Create tooltips for edges based on weight
    edge_weights <- get.edge.attribute.active(slice, "weight", at = onset)
    paste0(
      "Weight: ", edge_weights, "<br>"
    )
  },
  d3.options = list(
    playControls = TRUE,
    slider = TRUE,
    animationDuration = 800
  ),
  launchBrowser = TRUE      # Automatically open the animation in the browser
)
```

## Network Analysis

### Calculate the number of edges

```{r}
library(ggplot2)
library(networkDynamic)

# Initialize an empty data frame to store edge counts
edge_summary <- data.frame(Year = integer(), Edges = integer())

# Loop through each year and calculate the number of edges
for (year in 1980:2010) {
  # Extract the network slice for the year
  network_slice <- network.extract(dynamic_network, at = year)
  
  # Count the number of edges in the slice
  edge_count <- network.edgecount(network_slice)
  
  # Add the results to the summary data frame
  edge_summary <- rbind(edge_summary, data.frame(Year = year, Edges = edge_count))
}

# Plot the number of edges over the years
ggplot(edge_summary, aes(x = Year, y = Edges)) +
  geom_line(color = "blue", size = 1) +
  geom_point(color = "red", size = 2) +
  labs(
    title = "Number of Edges in the Network Over Years",
    x = "Year",
    y = "Number of Edges"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10)
  )
```
### Centrality analysis

```{r}
library(sna)
library(ggplot2)
library(igraph)

# Initialize a data frame to store centrality results
centrality_data <- data.frame(
  Year = integer(),
  Country = character(),
  Degree = numeric(),
  Betweenness = numeric(),
  Eigenvector = numeric(),
  Closeness = numeric()
)

# Loop through each year to calculate centrality metrics
for (year in 1980:2010) {
  # Extract the network slice for the current year
  network_slice <- network.extract(dynamic_network, at = year)
  
  # Retrieve node names for the current network slice
  node_names <- network.vertex.names(network_slice)
  
  # if slice is null then skip
  if (network.size(network_slice) == 0) {
    next
  }
  
  # Compute centrality metrics
  degree <- sna::degree(network_slice, gmode = "graph")
  betweenness <- sna::betweenness(network_slice, gmode = "graph")
  
  # Eigenvector
  adj_matrix <- as.matrix.network.adjacency(network_slice)  # extract adjacency matric
  eig <- eigen(adj_matrix)                                 # eigenvalue decomposition
  eigen_centrality <- abs(eig$vectors[, 1])                # The eigenvector corresponding to the maximum eigenvalue
  
  # Closeness
  g <- graph.adjacency(adj_matrix, mode = "undirected", weighted = NULL, diag = FALSE)
  closeness <- igraph::closeness(g, normalized = TRUE)
  
 # Store centrality data for the current year
  year_data <- data.frame(
    Year = rep(year, length(node_names)),   # Repeat the current year for each node
    Country = node_names,
    Degree = degree,
    Betweenness = betweenness,
    Eigenvector = eigen_centrality,
    Closeness = closeness
  )
  
  # Append the current year's data to the centrality_data data frame
  centrality_data <- rbind(centrality_data, year_data)
}

# Filter data to include only key countries (USA, China, Russia)
key_countries <- c("USA", "CHN", "RUS")
filtered_data <- centrality_data[centrality_data$Country %in% key_countries, ]

# Plot the temporal trends of Degree centrality
ggplot(filtered_data, aes(x = Year)) +
  geom_line(aes(y = Degree, color = Country), size = 1) +
  facet_wrap(~Country, scales = "free_y") +
  labs(title = "Degree Centrality Over Time",
       x = "Year",
       y = "Degree Centrality") +
  theme_minimal()
```
```{r}
# plot the temporal trends of Betweenness centrality
ggplot(filtered_data, aes(x = Year)) +
  geom_line(aes(y = Betweenness, color = Country), size = 1) +
  facet_wrap(~Country, scales = "free_y") +
  labs(title = "Betweenness Centrality Over Time",
       x = "Year",
       y = "Betweenness Centrality") +
  theme_minimal()
```
```{r}
# plot the temporal trends of Eigenvector centrality
ggplot(filtered_data, aes(x = Year)) +
  geom_line(aes(y = Eigenvector, color = Country), size = 1) +
  facet_wrap(~Country, scales = "free_y") +
  labs(title = "Eigenvector Centrality Over Time",
       x = "Year",
       y = "Eigenvector Centrality") +
  theme_minimal()
```
```{r}
# Closeness
ggplot(filtered_data, aes(x = Year)) +
  geom_line(aes(y = Closeness, color = Country), size = 1) +
  facet_wrap(~Country, scales = "free_y") +
  labs(title = "Closeness Centrality Over Time",
       x = "Year",
       y = "Closeness Centrality") +
  theme_minimal()
```
### 

```{r}
library(sna)
library(igraph)
library(ggplot2)
library(tidyr)
library(dplyr)

results <- list()
correlation_summary <- data.frame()

for (year in 1982:2010) {
  # Extract the network slice for the current year
  network_slice <- network.extract(dynamic_network, at = year)
  
  # Retrieve node names and military spending data for the current year
  node_names <- network.vertex.names(network_slice)
  military_spending <- get.vertex.attribute.active(dynamic_network, "military_spending_to_GDP", at = year)
  
  # Skip the year if the network slice is empty
  if (network.size(network_slice) == 0) {
    next
  }
  
  # Compute Degree and Betweenness centrality
  degree <- sna::degree(network_slice, gmode = "graph")
  betweenness <- sna::betweenness(network_slice, gmode = "graph")
  
  # Compute Eigenvector centrality using adjacency matrix
  adj_matrix <- as.matrix.network.adjacency(network_slice) 
  eig <- eigen(adj_matrix)                                 
  eigen_centrality <- abs(eig$vectors[, 1])                
  
  # Compute Closeness centrality for the largest connected component
  g <- graph.adjacency(adj_matrix, mode = "undirected", weighted = NULL, diag = FALSE)
  components <- components(g) 
  largest_component <- which.max(components$csize) 
  subgraph <- induced_subgraph(g, which(components$membership == largest_component)) # Extract subgraph
  
  # Compute Closeness centrality within the largest connected component
  closeness <- rep(NA, length(node_names)) # 初始化为 NA
  subgraph_closeness <- igraph::closeness(subgraph, normalized = TRUE)
  
  # 将 closeness 结果映射回所有节点
  subgraph_nodes <- V(subgraph)$name
  subgraph_indices <- match(subgraph_nodes, node_names) # Map subgraph nodes to original network
  closeness[subgraph_indices] <- subgraph_closeness # Assign subgraph closeness values
  
  # Create a data frame for the current year's results
  year_data <- data.frame(
    Country = node_names,
    Degree = degree,
    Betweenness = betweenness,
    Eigenvector = eigen_centrality,
    Closeness = closeness,
    MilitarySpendingGDP = military_spending,
    Year = year
  )
  
  # Filter nodes with missing military spending data
  year_data <- year_data %>% filter(!is.na(MilitarySpendingGDP))
  
  
  # Compute correlations only if there are sufficient data points
  if (nrow(year_data) > 1) {
    # 计算相关性
    cor_degree <- cor(year_data$Degree, year_data$MilitarySpendingGDP)
    cor_betweenness <- cor(year_data$Betweenness, year_data$MilitarySpendingGDP)
    cor_eigenvector <- cor(year_data$Eigenvector, year_data$MilitarySpendingGDP)
    cor_closeness <- cor(year_data$Closeness, year_data$MilitarySpendingGDP, use = "complete.obs")
  } else {
    cor_degree <- NA
    cor_betweenness <- NA
    cor_eigenvector <- NA
    cor_closeness <- NA
  }
  
  # Store the results for the current year
  results[[as.character(year)]] <- list(
    Data = year_data,
    Correlation = data.frame(
      Year = year,
      DegreeCorrelation = cor_degree,
      BetweennessCorrelation = cor_betweenness,
      EigenvectorCorrelation = cor_eigenvector,
      ClosenessCorrelation = cor_closeness
    )
  )
  
  # Append the current year's correlations to the summary
  correlation_summary <- rbind(
    correlation_summary,
    data.frame(
      Year = year,
      DegreeCorrelation = cor_degree,
      BetweennessCorrelation = cor_betweenness,
      EigenvectorCorrelation = cor_eigenvector,
      ClosenessCorrelation = cor_closeness
    )
  )
}


# Reshape the correlation summary data frame to long format for visualization
correlation_summary_long <- correlation_summary %>%
  pivot_longer(
    cols = c(DegreeCorrelation, BetweennessCorrelation, EigenvectorCorrelation, ClosenessCorrelation),
    names_to = "CentralityType",  # Column for centrality type
    values_to = "Correlation"   # Column for correlation values
  )


# Plot correlation trends over time
ggplot(correlation_summary_long, aes(x = Year, y = Correlation, color = CentralityType)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Centrality and change in the correlation of military expenditure /GDP",
    x = "Year",
    y = "Correlation",
    color = "Centrality Type"
  ) +
  theme_minimal()
```

### Community Detection

This part performs Louvain community detection on a dynamic network (year by year), aligns the community labels across years via the Hungarian Algorithm (using the clue package), and then visualizes both the raw and the consistent community transitions for specific countries. This process helps track how each node’s community membership evolves consistently over time.

```{r}
library(igraph)
library(networkDynamic)
library(dplyr)
# install.packages("clue")
library(clue)    # Provides solve_LSAP() function (Hungarian Algorithm for linear assignment)

# 1. Louvain community detection (year by year)
community_results <- list()

for (year in 1980:2010) {
  # Extract the network slice for the specified year
  network_slice <- network.extract(dynamic_network, at = year)
  
  # Retrieve active edges for this year
  edge_ages <- dyads.age.at(network_slice, at = year, format.out = "edgelist")
  
  if (is.null(edge_ages) || nrow(edge_ages) == 0) {
    cat("Year", year, "has no active edges, skipping.\n")
    next
  }
  
  # Obtain node names
  node_names <- network.vertex.names(network_slice)
  
  # Build an edge list (using node names)
  edge_list_named <- data.frame(
    tails = node_names[edge_ages[, 1]],
    heads = node_names[edge_ages[, 2]],
    stringsAsFactors = FALSE
  )
  
  # Create an igraph object & remove isolated nodes
  g <- graph_from_data_frame(edge_list_named, directed = FALSE)
  g <- delete.vertices(g, which(degree(g) == 0))
  
  if (vcount(g) == 0 || ecount(g) == 0) {
    cat("Year", year, "has no connected components after removing isolates, skipping.\n")
    next
  }
  
  # Louvain community detection
  community <- cluster_louvain(g)
  mem <- membership(community)
  
  community_results[[as.character(year)]] <- list(
    graph = g,
    communities = mem,
    modularity = modularity(community)
  )
  
  cat("Year", year, "processed with", length(unique(mem)), "communities.\n")
}

# 2. Merge all yearly community detection results into community_changes
community_changes <- data.frame(
  Year = integer(),
  Country = character(),
  Community = integer(),
  stringsAsFactors = FALSE
)

for (year in names(community_results)) {
  # membership is a vector, names(membership) are node names, values are community labels
  mem <- community_results[[year]]$communities
  
  df_year <- data.frame(
    Year = as.integer(year),
    Country = names(mem),
    Community = as.integer(mem),
    stringsAsFactors = FALSE
  )
  
  community_changes <- rbind(community_changes, df_year)
}

# 3. Analyze frequent switches (based on original labels)
community_transitions <- community_changes %>%
  group_by(Country) %>%
  summarise(Transitions = n_distinct(Community)) %>%
  arrange(desc(Transitions))

cat("\nTop 10 Countries by raw 'Community' transitions (without label alignment):\n")
print(head(community_transitions, 10))

# Pick the top 10 countries with the most community changes
top_countries <- community_transitions %>%
  slice(1:10) %>%
  pull(Country)

# Only visualize community transitions for these top 10 countries (original labels)
top_community_changes <- community_changes %>%
  filter(Country %in% top_countries)

# 4. Plot original community transitions (no label alignment)
library(ggplot2)

ggplot(top_community_changes, aes(x = Year, y = Community, group = Country, color = Country)) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  labs(
    title = "Raw Community Transitions of Top 10 Countries (1980-2010)",
    x = "Year",
    y = "Raw Community Label"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    text = element_text(size = 14)
  )

####### Improving label consistency with the Hungarian algorithm #######
# Introduce Hungarian assignment (solve_LSAP) to align community labels year to year.

# 5. Group all nodes by year -> compute a dictionary of "community members" per year
all_years <- sort(unique(community_changes$Year))
community_membership <- list()
for (y in all_years) {
  this_year <- community_changes %>% filter(Year == y)
  # Split by Community ID -> each community's list of members
  community_membership[[as.character(y)]] <- split(this_year$Country, this_year$Community)
}

# 6. Add a new column `ConsistentCommunity` (initially same as Community)
community_changes <- community_changes %>%
  mutate(ConsistentCommunity = Community)

# 7. Build Jaccard matrices & use Hungarian to align communities from year-1 to year
for (i in 2:length(all_years)) {
  year_cur <- all_years[i]
  year_prev <- all_years[i - 1]
  
  prev_comm_list <- community_membership[[as.character(year_prev)]]
  curr_comm_list <- community_membership[[as.character(year_cur)]]
  
  # Skip if either year has no communities (edge cases)
  if (length(prev_comm_list) == 0 || length(curr_comm_list) == 0) {
    next
  }
  
  # Construct Jaccard matrix: rows = previous year's communities, cols = current year's communities
  matrix_jaccard <- matrix(0, 
                           nrow = length(prev_comm_list),
                           ncol = length(curr_comm_list))
  
  rownames(matrix_jaccard) <- names(prev_comm_list) 
  colnames(matrix_jaccard) <- names(curr_comm_list)
  
  for (r in seq_along(prev_comm_list)) {
    for (c in seq_along(curr_comm_list)) {
      intersect_size <- length(intersect(prev_comm_list[[r]], curr_comm_list[[c]]))
      union_size <- length(union(prev_comm_list[[r]], curr_comm_list[[c]]))
      if (union_size > 0) {
        matrix_jaccard[r, c] <- intersect_size / union_size
      } else {
        matrix_jaccard[r, c] <- 0
      }
    }
  }
  
  # Use Hungarian to maximize Jaccard -> transform similarity into cost = 1 - Jaccard
  cost_matrix <- 1 - matrix_jaccard
  
  # If row != col, pad the matrix to become square
  nr <- nrow(cost_matrix)
  nc <- ncol(cost_matrix)
  if (nr > nc) {
    cost_matrix <- cbind(cost_matrix, matrix(1, nrow = nr, ncol = nr - nc))
  } else if (nc > nr) {
    cost_matrix <- rbind(cost_matrix, matrix(1, nrow = nc - nr, ncol = nc))
  }
  
  assignment <- solve_LSAP(cost_matrix)
  
  # Build a mapping: "current year's community" -> "previous year's community"
  original_nc <- ncol(matrix_jaccard)
  
  row_labels <- rownames(matrix_jaccard)
  col_labels <- colnames(matrix_jaccard)
  
  matched_mapping <- rep(NA, original_nc)
  names(matched_mapping) <- col_labels
  
  for (r in seq_len(nrow(matrix_jaccard))) {
    c_assigned <- assignment[r]
    if (c_assigned <= original_nc) {
      matched_mapping[col_labels[c_assigned]] <- row_labels[r]
    }
  }
  
  # Apply matched_mapping to update ConsistentCommunity
  community_changes <- community_changes %>%
    mutate(CommChar = as.character(Community))
  
  prev_lookup_df <- community_changes %>%
    filter(Year == year_prev) %>%
    distinct(CommChar, ConsistentCommunity) %>%
    rename(PrevCommunityLabel = CommChar,
           PrevConsistent = ConsistentCommunity)
  
  current_year_df <- community_changes %>% filter(Year == year_cur)
  current_year_df <- current_year_df %>%
    rowwise() %>%
    mutate(
      MappedPrevComm = matched_mapping[CommChar],
      ConsistentCommunity = if(!is.na(MappedPrevComm)) {
        tmp_cc <- prev_lookup_df %>% 
          filter(PrevCommunityLabel == MappedPrevComm) %>%
          pull(PrevConsistent)
        if(length(tmp_cc) == 1) tmp_cc else ConsistentCommunity
      } else {
        ConsistentCommunity
      }
    ) %>%
    ungroup()
  
  community_changes <- community_changes %>%
    filter(Year != year_cur) %>%
    bind_rows(current_year_df)
  
  # We could update community_membership[[year_cur]] here if needed
}

# Sort final results
community_changes <- community_changes %>%
  arrange(Year, Country)

# 8. Visualize the consistent community transitions for the same top_countries
top_community_changes_consistent <- community_changes %>%
  filter(Country %in% top_countries)

ggplot(top_community_changes_consistent, aes(x = Year, y = ConsistentCommunity, color = Country, group = Country)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(
    title = "Consistent Community Transitions (Hungarian Matching)",
    x = "Year",
    y = "Consistent Community Label"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank(),
    text = element_text(size = 14)
  )

# Inspect final data
head(community_changes, 20)

# If you only want to focus on five specific countries:
focus_countries <- c("JOR","CHN","IRN","TUR","DRC")

five_community_changes <- community_changes %>%
  filter(Country %in% focus_countries)

five_community_changes

# Plot consistent community transitions for the 5 chosen countries
library(ggplot2)

ggplot(five_community_changes, aes(x = Year, y = ConsistentCommunity, color = Country, group = Country)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(
    title = "Consistent Community Transitions for Selected Countries",
    x = "Year",
    y = "Consistent Community Label"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "bottom",
    legend.title = element_blank()
  )

```
### Brokerage analysis
performs a dynamic brokerage analysis using the brokerage() function from the sna package. For each year between 1980 and 2010, it extracts a network slice, classifies nodes into groups (HighSpender vs. LowSpender vs. Unknown) based on their military spending to GDP ratio, and computes Gould-Fernandez brokerage statistics (w_I, w_O, b_IO, b_OI, b_O, t). Then, for each year, it identifies the top broker (the node with the highest t value). Finally, it visualizes these yearly top brokers as a bar chart, using a pastel color palette for a more visually pleasing output.

```{r}
library(dplyr)
library(networkDynamic)
library(sna)

# dynamic_network is already constructed, containing "military_spending_to_GDP"
threshold <- 2  # Example threshold: greater than 2% is considered HighSpender

brokerage_summary <- data.frame()

for (year in 1980:2010) {
  # 1. Extract the network slice for the given year
  slice <- network.extract(dynamic_network, at = year)
  
  # Skip if there are insufficient nodes/edges
  if (network.size(slice) < 2 || network.edgecount(slice) == 0) {
    cat("Year", year, "insufficient nodes/edges, skipping.\n")
    next
  }
  
  node_names <- network.vertex.names(slice)
  if (is.null(node_names) || length(node_names) == 0) {
    cat("Year", year, "no node names, skipping.\n")
    next
  }
  
  # 2. Retrieve military spending to GDP
  mil_spending <- get.vertex.attribute.active(
    dynamic_network, 
    "military_spending_to_GDP",
    at = year
  )
  
  # Skip if all values are NA
  if (all(is.na(mil_spending))) {
    cat("Year", year, "all NA in mil_spending, skipping.\n")
    next
  }
  
  # Ensure length matches the node list
  if (length(mil_spending) < length(node_names)) {
    mil_spending <- c(mil_spending, rep(NA, length(node_names) - length(mil_spending)))
  }
  
  # 3. Group classification
  group_vector <- ifelse(mil_spending > threshold, "HighSpender", "LowSpender")
  group_vector[is.na(group_vector)] <- "Unknown"
  
  # 4. Call brokerage() (no roles/complete parameters available)
  bres <- brokerage(slice, cl = group_vector)
  
  z_matrix <- bres$z.nli  # Standardized Z scores => w_I, w_O, b_IO, b_OI, b_O, t
  if (ncol(z_matrix) == 0) {
    cat("Year", year, "z.nli is empty, skipping.\n")
    next
  }
  
  z_df <- as.data.frame(z_matrix)
  # Expected columns: c("w_I","w_O","b_IO","b_OI","b_O","t")
  # 't' is the sum of the first 5 columns' Z scores
  
  z_df$Country <- node_names
  z_df$Year <- year
  
  # 5. Identify the node with the highest 't' value
  if (!"t" %in% colnames(z_df)) {
    cat("Year", year, "has no 't' column, skipping.\n")
    next
  }
  
  top_broker <- z_df %>%
    arrange(desc(t)) %>%
    slice_head(n = 1)  # Only select the node with the highest t value
  
  brokerage_summary <- rbind(brokerage_summary, top_broker)
  
  cat("Year", year, "processed. top-broker =", top_broker$Country, 
      " t =", top_broker$t, "\n")
}

# Convert Year to factor for better display on discrete axes
brokerage_summary <- brokerage_summary %>%
  mutate(Year = factor(Year))

# Draw a bar chart, one bar for each year
library(RColorBrewer)
library(ggplot2)

ggplot(brokerage_summary, aes(x = Year, y = t, fill = Country)) +
  geom_bar(stat = "identity", width = 0.7) +
  geom_text(
    aes(label = Country), 
    color = "black",             
    size = 2, 
    fontface = "bold",
    position = position_stack(vjust = 0.5)
  ) +
  scale_fill_brewer(palette = "Pastel1") +  # Soft color palette
  labs(
    title = "Yearly Top Broker by 't' Value",
    x = "Year",
    y = "Brokerage Z-score (t)"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5),
    legend.position = "none"
  )

```

